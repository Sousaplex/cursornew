# Gemini API Usage Guide for AI Assistants

This document provides rules and guidelines for interacting with the Google Gemini API, focusing on the Python SDK.

## 1. Core Concepts

-   **Client Initialization**: Always start by initializing the `genai.Client` with an API key. The key should be stored securely, preferably in a `.env.local` file and loaded using `python-dotenv`.
-   **Model Selection**: Choose the appropriate model for the task (e.g., `gemini-1.5-flash` for speed, `gemini-1.5-pro` for complex reasoning).
-   **Content Generation**: Use the `client.models.generate_content()` method for single-turn requests and `client.chats.create()` for multi-turn conversations.

## 2. Authentication and Setup

The API key is required for all calls. Use `dotenv` to load it from a `.env.local` file.

```python
import os
from dotenv import load_dotenv
import google.genai as genai

# Load environment variables from .env.local
load_dotenv(dotenv_path=Path('.') / '.env.local')

# Configure the client with the API key
api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    raise ValueError("GEMINI_API_KEY not found in environment variables.")

# In google.genai v0.7.0, the client is initialized directly
# For older versions, genai.configure(api_key=api_key) was used.
# The current project uses the client interface.
client = genai.Client(api_key=api_key)
```

## 3. Text Generation

### 3.1. Simple Text Prompt

For straightforward text-only prompts, pass a list containing the text string to the `contents` parameter.

```python
import google.genai as genai

# Assumes client is already initialized
response = client.models.generate_content(
    model="gemini-2.5-pro-preview-06-05",
    contents=["Explain how a transformer model works in a neural network."]
)
print(response.text)
```

### 3.2. Streaming Responses

For interactive applications or to handle long responses, use `generate_content_stream()` to receive the response in chunks as it's being generated.

```python
import google.genai as genai

# Assumes client is already initialized
response_stream = client.models.generate_content_stream(
    model="gemini-2.5-pro-preview-06-05",
    contents=["Write a short story about a programmer who discovers a secret in the code."]
)

for chunk in response_stream:
    print(chunk.text, end="")
```

## 4. Configuration and Customization

Use the `generation_config` object to control the model's output.

### 4.1. Key Parameters

-   **`temperature`**: Controls randomness. Lower values (e.g., `0.2`) are more deterministic, while higher values (e.g., `0.9`) are more creative. A value of `0` is deterministic.
-   **`max_output_tokens`**: Sets the maximum length of the response.
-   **`top_p`**, **`top_k`**: Control the nucleus and top-k sampling methods.
-   **`response_mime_type`**: Use `"application/json"` to request a JSON response, which is useful for structured output.

### 4.2. Applying Configuration

Pass a `generation_config` dictionary when calling the model.

```python
import google.genai as genai

# Assumes client is already initialized
generation_config = {
    "temperature": 0.4,
    "max_output_tokens": 2048,
    "response_mime_type": "application/json"
}

# The prompt should instruct the model to generate JSON
prompt = """
Based on the following text, create a JSON object with 'summary' and 'keywords' fields.
Text: The quick brown fox jumps over the lazy dog. It was a demonstration of agility and speed.
"""

response = client.models.generate_content(
    model="gemini-2.5-pro-preview-06-05",
    contents=[prompt],
    generation_config=generation_config
)

print(response.text)
```

### 4.3. System Instructions

Use `system_instruction` to set a persona or provide high-level guidance for the model's behavior across a request.

```python
import google.genai as genai
from google.genai import types

# Assumes client is already initialized

# In google.genai v0.7.0, system instructions are part of the config
system_instruction = "You are a helpful coding assistant who provides answers in Python."

response = client.models.generate_content(
    model="gemini-2.5-pro-preview-06-05",
    system_instruction=system_instruction,
    contents=["How do I create a list in Python?"]
)

print(response.text)
```

## 5. Multimodal Capabilities and the Files API

Gemini models can process multiple types of data (modes), including text, images, audio, and documents. For including media in prompts, you have two options: direct input for small files or the Files API for larger, persistent, or reusable files. For more details, refer to the [official Files API documentation](https://ai.google.dev/gemini-api/docs/files?authuser=1).

### 5.1. Direct Media Input

For smaller files (under 20MB in a request), you can pass media objects directly into the `contents` list. This is common for simple image analysis.

```python
from PIL import Image
import google.genai as genai

# Assumes client is already initialized
try:
    img = Image.open("path/to/your/image.jpg")

    response = client.models.generate_content(
        model="gemini-2.5-pro-preview-06-05", # Use a model that supports vision
        contents=[img, "Describe what you see in this image."]
    )
    print(response.text)
except FileNotFoundError:
    print("Image file not found. Please update the path.")
```

### 5.2. Using the Files API for Large or Reusable Media

The Files API is the recommended method for handling files, especially when the total request size exceeds 20MB or when you intend to reuse a file across multiple prompts. Uploaded files are automatically deleted after 2 days.

The workflow involves four main steps: upload, check state, use in a prompt, and (optionally) delete.

#### 5.2.1. Uploading a File

Use `client.files.upload()` to upload your media. This call returns a `File` object that you will reference in subsequent steps.

```python
import google.genai as genai

# Assumes client is already initialized
video_path = "path/to/your/video.mp4"

try:
    print(f"Uploading file: {video_path}...")
    video_file = client.files.upload(file=video_path)
    print(f"Completed upload: {video_file.name}")
except FileNotFoundError:
    print(f"File not found at {video_path}")
    video_file = None
```

#### 5.2.2. Checking File State

After uploading, a file enters a `PROCESSING` state. You must wait for it to become `ACTIVE` before using it in a prompt. You can check the status using `client.files.get()`.

```python
import time
import google.genai as genai

# Assumes client and video_file object from the previous step exist
if video_file:
    while video_file.state.name == "PROCESSING":
        print("Waiting for file to be processed...", end="\r")
        time.sleep(10)
        video_file = client.files.get(name=video_file.name)

    if video_file.state.name == "FAILED":
        raise ValueError(f"File processing failed: {video_file.state.name}")

    print(f"File is now {video_file.state.name.lower()}.")
```

#### 5.2.3. Using the Uploaded File in a Prompt

Once the file is `ACTIVE`, pass the `File` object directly into the `contents` list of your `generate_content` call.

```python
import google.genai as genai

# Assumes client and video_file object are available and active
if video_file and video_file.state.name == "ACTIVE":
    prompt = "Describe the first 5 seconds of this video."
    response = client.models.generate_content(
        model="gemini-2.5-pro-preview-06-05",
        contents=[prompt, video_file] # Pass the file object
    )
    print("\n--- Model Response ---")
    print(response.text)
```

#### 5.2.4. Managing Files

You can list all your uploaded files or delete a specific file once you are finished with it.

```python
import google.genai as genai

# Assumes client is initialized

# List all files
print("\n--- All Uploaded Files ---")
for f in client.files.list():
    print(f"  - {f.name} ({f.display_name})")

# Delete a specific file
if video_file:
    try:
        client.files.delete(name=video_file.name)
        print(f"\nSuccessfully deleted file: {video_file.name}")
    except Exception as e:
        print(f"Error deleting file: {e}")
```

## 6. Multi-Turn Conversations (Chat)

For conversational interfaces, use the chat functionality to maintain history. The SDK handles sending the conversation history with each new message.

```python
import google.genai as genai

# Assumes client is already initialized
chat = client.chats.create(model="gemini-2.5-pro-preview-06-05")

# First turn
response = chat.send_message("Hello! I'm creating a Python script.")
print("AI: ", response.text)

# Second turn
response = chat.send_message("What's a good library for making HTTP requests?")
print("AI: ", response.text)

# You can inspect the history
# for message in chat.history:
#     print(f"**{message.role}**: {message.parts[0].text}")
```

## 7. Advanced Capabilities: Gemini Thinking

The Gemini 2.5 series models (e.g., `gemini-2.5-pro-preview-06-05`) feature an internal "thinking" process that enhances their reasoning and planning capabilities for complex tasks. This feature is enabled by default. For more details, refer to the [official documentation on Gemini Thinking](https://ai.google.dev/gemini-api/docs/thinking).

### 7.1. Thought Summaries

You can request summaries of the model's internal thought process. This is useful for debugging the model's reasoning or providing progress updates to users during long-running tasks.

To enable this, set `include_thoughts=True` within a `thinking_config` object.

```python
import google.genai as genai
from google.genai import types

# Assumes client is already initialized
prompt = "What is the sum of the first 50 prime numbers?"
response = client.models.generate_content(
  model="gemini-2.5-pro-preview-06-05", # A model with thinking capability
  contents=prompt,
  generation_config=types.GenerationConfig(
    thinking_config=types.ThinkingConfig(
      include_thoughts=True
    )
  )
)

# The response will contain both the final answer and the thought summary
for part in response.candidates[0].content.parts:
  if part.thought:
    print("--- Thought Summary ---")
    print(part.text)
    print("-----------------------")
  else:
    print("--- Final Answer ---")
    print(part.text)
    print("--------------------")

# You can also access the token count for thoughts
print(f"Thoughts tokens: {response.usage_metadata.thoughts_token_count}")
print(f"Output tokens: {response.usage_metadata.candidates_token_count}")
```

### 7.2. Managing the Thinking Budget

For complex tasks, you might want to give the model more "thinking time" by allocating a larger token budget for its internal reasoning process. This is done via the `thinking_budget` parameter. A higher budget can lead to better results on difficult problems but may increase latency and cost.

```python
import google.genai as genai
from google.genai import types

# Assumes client is already initialized
# Allocate more tokens for the thinking process
thinking_budget = 2048 # Default is 1024

response = client.models.generate_content(
  model="gemini-2.5-pro-preview-06-05",
  contents=["Write Python code for a web application that visualizes real-time stock market data."],
  generation_config=types.GenerationConfig(
    thinking_config=types.ThinkingConfig(
      thinking_budget=thinking_budget
    )
  )
)

print(response.text)
```

### 7.3. Pricing for Thinking

When using thinking models, the total cost is the sum of the output tokens and the thinking tokens. The number of thinking tokens used is available in the `usage_metadata` of the response as `thoughts_token_count`.

This guide covers the primary use cases. For more advanced features like function calling or fine-tuning, refer to the official Google AI for Developers documentation. 